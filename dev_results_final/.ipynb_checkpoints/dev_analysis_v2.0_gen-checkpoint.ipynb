{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Genuine files in dev set - version 2.0 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    " * Draw scatter plot of posteriors for our best CNN system to project the distribution of genuine and spoofed features\n",
    " * Do this for Development and evaluation set.\n",
    " * On Evaluation set you could do this plot seperately for different Replay configurations (total 57 will be too much though)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of file list for spoofed and genuine files in Development set for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) The top 10 genuine confident correctly classified files : high scores\n",
    "\n",
    "                               Gen prob     Spf Prob      Log Liklihood ratio\n",
    "                                    \n",
    "       \n",
    "**** Note when accessing via list or array, index 575 should be called as 574       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_train/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "ls /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_train/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the prediction file\n",
    "# Predictions file that has three columns: genuin, spoofed prob and log likehood ratio\n",
    "\n",
    "pred_file='model_3sec_relu_0.5_run8/predictions/dev_prediction.txt'\n",
    "#new_pred_file = 'model_3sec_relu_0.5_run8/predictions/dev_prediction_with_index.txt'\n",
    "new_pred_file = 'predictions/dev_prediction_with_index.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open(pred_file) as f, open(new_pred_file,'w') as f2:\\n    i=0\\n    for line in f:\\n        f2.write(str(i)+' '+ line)\\n        #print(i)\\n        i+=1    \\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new prediction file where we write index (starting from 0) as the first column\n",
    "# This way things become much easier\n",
    "\n",
    "'''\n",
    "with open(pred_file) as f, open(new_pred_file,'w') as f2:\n",
    "    i=0\n",
    "    for line in f:\n",
    "        f2.write(str(i)+' '+ line)\n",
    "        #print(i)\n",
    "        i+=1    \n",
    "'''\n",
    "\n",
    "# Above code is just used to append file index in the prediction file\n",
    "# Careful when re-running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.759194 0.240806 1.14826\n",
      "1 0.833182 0.166818 1.60835\n",
      "2 0.704341 0.295659 0.868054\n",
      "3 0.435833 0.564167 -0.25809\n",
      "4 0.143625 0.856375 -1.7855\n",
      "5 0.966915 0.0330851 3.37503\n",
      "6 0.693761 0.306239 0.81776\n",
      "7 0.946258 0.0537418 2.86832\n",
      "8 0.952323 0.0476765 2.99447\n",
      "9 0.9263 0.0737001 2.53119\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat predictions/dev_prediction_with_index.txt | head\n",
    "\n",
    " #Check the new prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out predictions of genuine and spoofed files\n",
    "\n",
    "all_predictions = []\n",
    "all_gens = []\n",
    "all_spoofs = []\n",
    "\n",
    "with open(new_pred_file) as f:\n",
    "    all_predictions = [line.strip() for line in f]\n",
    "    \n",
    "all_gen_predictions = all_predictions[:760]                    # first 760 files in dev are genuine\n",
    "all_spf_predictions = all_predictions[760:]                  # files from 761-1710 are spoofed examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "print(len(all_gen_predictions))\n",
    "print(len(all_spf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfname='index_file_list/allGenIndexList_TP.txt'\\ncount=0\\nsplitIndex = 1     # 1 for genuine and 2 for spoofed (the third column)\\npredList = all_gen_predictions   # use all_spf_predictions for spoofed case\\n\\n#print(len(predList))\\n\\nwith open(fname,'w') as f2:\\n    for items in predList:\\n        prob = items.strip().split(' ')[splitIndex]   \\n        if float(prob) > 0.9:\\n            #print(items)\\n            count+=1\\n            f2.write(items+'\\n')            \\nprint(count)\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect all genuine file for which genuine class got probability > 90%\n",
    "# the strongly correctly classified cases\n",
    "\n",
    "'''\n",
    "fname='index_file_list/allGenIndexList_TP.txt'\n",
    "count=0\n",
    "splitIndex = 1     # 1 for genuine and 2 for spoofed (the third column)\n",
    "predList = all_gen_predictions   # use all_spf_predictions for spoofed case\n",
    "\n",
    "#print(len(predList))\n",
    "\n",
    "with open(fname,'w') as f2:\n",
    "    for items in predList:\n",
    "        prob = items.strip().split(' ')[splitIndex]   \n",
    "        if float(prob) > 0.9:\n",
    "            #print(items)\n",
    "            count+=1\n",
    "            f2.write(items+'\\n')            \n",
    "print(count)\n",
    "\n",
    "'''\n",
    "\n",
    "# Careful when you re-run this code. It is use to take files with 90% probability to do analysis using slime !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total genuine files we got with > 90% probability is = 685 (out of 760)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Take the top 10 genuine confident correctly classified files - high scores\n",
    "\n",
    "                               Gen prob     Spf Prob      Log Liklihood ratio\n",
    "    \n",
    "       \n",
    "**** Note when accessing via list or array, index 575 should be called as 574       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analysing Time: The top two components from SLIME - True positive Genuine case\n",
    "\n",
    "Note, that under time analysis, we have cut our input spectrogram into 10 different temporal components/segments, where each segment correpsonds to\n",
    "\n",
    "> ***300 mili seconds***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "time.png",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show how we cut the spectrogram in timexfrequency\n",
    "\n",
    "Image(\"time.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the temporal explanantion file produced by slime\n",
    "\n",
    "file = 'top_two_explanation_indices/time/gen_TP_box.txt'    #using box for spectrogram computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Make sure to use the correct spectrogram for two cases !! ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cat top_two_explanation_indices/time/gen_TP_box.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top1_top2_list(file):\n",
    "    with open(file) as f:\n",
    "        top1 = [int(line.strip().split(' ')[0]) for line in f]\n",
    "    with open(file) as f:  \n",
    "        top2 = [int(line.strip().split(' ')[1]) for line in f]\n",
    "    return top1, top2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top1 and top2 in seperate list\n",
    "top1, top2 = get_top1_top2_list(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n",
      "685\n"
     ]
    }
   ],
   "source": [
    "print(len(top1))\n",
    "print(len(top2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_counts(datalist, key):\n",
    "    count=0\n",
    "    for i in range(0,len(datalist)):\n",
    "        if datalist[i] == key:\n",
    "            count+=1\n",
    "    return count            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_component_distribution(comps, predList, order):\n",
    "    print('Printing component weigting distribution for Top:', order)\n",
    "    for i in comps:\n",
    "        print('Component ' + str(i) + ' : ' + str(get_counts(predList, i)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the top1 components - given by SLIME (Time segmentation) - Genuine Class\n",
    "\n",
    "   \n",
    "> Using only those files with 90% above genuine class probability. The pattern is same (total files this time we used is 685 earlier we used 766 that include genuine files with proba 60 % also\n",
    "\n",
    "    Component 0 : 18\n",
    "    Component 1 : 0\n",
    "    Component 2 : 112\n",
    "    Component 3 : 247\n",
    "    Component 4 : 31\n",
    "    Component 5 : 160\n",
    "    Component 6 : 95\n",
    "    Component 7 : 21\n",
    "    Component 8 : 1\n",
    "    Component 9 : 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comps=[0,1,2,3,4,5,6,7,8,9]    # In time we have 10 segments/components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing component weigting distribution for Top: 1\n",
      "Component 0 : 18\n",
      "Component 1 : 0\n",
      "Component 2 : 112\n",
      "Component 3 : 247\n",
      "Component 4 : 31\n",
      "Component 5 : 160\n",
      "Component 6 : 95\n",
      "Component 7 : 21\n",
      "Component 8 : 1\n",
      "Component 9 : 0\n"
     ]
    }
   ],
   "source": [
    "# Print distribution on top1\n",
    "\n",
    "order = 1\n",
    "print_component_distribution(comps, top1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we see that from 766 genuine TP audio files (that got scores > 0.5) we find that the largest number of times, SLIME gives highest weightage to Component 4 (271 times out of 766). Component 4 corresponds to 900-1200ms audio signal. This is then followed by Component 1 (136) which is the first 300 ms signal.\n",
    "\n",
    "Using signal Box also we found the same behaviour.\n",
    "\n",
    " * What to do next?\n",
    "We now pick those 271 audio files that got top1 as component 4 and try to find if we get some meaningful information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick 5 audio files for hearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random_5_ids = [506,480,757,208,481]   # Genuine True positive, >90% probability\n",
    "#add 1 to each to access correct file index in real world\n",
    "\n",
    "#random_5_ids = [507,481,758,209,482]   # Genuine True positive, >90% probability\n",
    "#base='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#ls /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_dev/D_1000481.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#cp /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_dev/D_1000482.wav audio_files_hearing/genuine_TP/\n",
    "\n",
    "#play audio_files_hearing/genuine_TP/D_1000507.wav\n",
    "#ls audio_files_hearing/genuine_TP/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What have we done so far\n",
    "\n",
    " 1. We took all genuine files that were correctly classified by CNN with more than 90% probability. We obtained 685 genuine files.\n",
    " 1. Then we applied SLIME. We segmented along time. We partitioned these files in 10 parts, each of 300ms. We then obtained top 2 influencing component index for all these files.\n",
    " 1. We then analysed the explanation index distribution for these 685 files and find that the component 4 that corresponds to 900-1200ms audio are maximally activated (got highest count = 247/685)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analysing Freq: The top two components from SLIME - True positive Genuine case\n",
    "\n",
    "Note, that under frequency analysis, we have cut our input spectrogram into 8 different frequency components/segments, where each segment correpsonds to\n",
    "\n",
    "> ***1000 Hz frequency***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "frequency.png",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show how we cut the spectrogram in timexfrequency\n",
    "\n",
    "Image(\"frequency.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the spectral explanation file\n",
    "\n",
    "file = 'top_two_explanation_indices/freq/gen_TP_box.txt'   #using signal box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#cat 'top_two_explanation_indices/freq/gen_TP.txt' | head\n",
    "\n",
    "# the top two components 7 6 dominates the explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top1 and top2 in seperate list\n",
    "top1, top2 = get_top1_top2_list(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n",
      "685\n"
     ]
    }
   ],
   "source": [
    "print(len(top1))\n",
    "print(len(top2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 component distribution - Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comps = [0,1,2,3,4,5,6,7]   # in Frequency we have 8 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing component weigting distribution for Top: 1\n",
      "Component 0 : 2\n",
      "Component 1 : 0\n",
      "Component 2 : 0\n",
      "Component 3 : 0\n",
      "Component 4 : 0\n",
      "Component 5 : 9\n",
      "Component 6 : 0\n",
      "Component 7 : 674\n"
     ]
    }
   ],
   "source": [
    "# Print distribution on top1\n",
    "\n",
    "order = 1\n",
    "print_component_distribution(comps, top1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With 685 files chosen with > 90% probability, distribution of TOP1 explanation (its same as before)\n",
    "\n",
    "    Component 0 : 2\n",
    "    Component 1 : 0\n",
    "    Component 2 : 0\n",
    "    Component 3 : 0\n",
    "    Component 4 : 0\n",
    "    Component 5 : 9\n",
    "    Component 6 : 0\n",
    "    Component 7 : 674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get those file index having component8 in top explanation (as for frequency we get top explanation at index8)\n",
    "top = 7\n",
    "\n",
    "fname='index_file_list/allGenIndexList_TP.txt'\n",
    "with open(fname) as f:\n",
    "    #file_idxs = [int(line.strip().split(' ')[0]) for line in f]\n",
    "    file_idxs = [line.strip() for line in f]\n",
    "    \n",
    "#Write the top component index to the file and further analysis\n",
    "fname='top_two_explanation_indices/freq/topExplanation_list_gen_TP_box.txt'\n",
    "id8_indexFiles=list() \n",
    "\n",
    "f=open(fname,'w')\n",
    "for i in range(len(top1)):\n",
    "    if top1[i]==top:    # if top explanation index is 3 (which is actually 4)\n",
    "        id8_indexFiles.append(file_idxs[i])\n",
    "        #print(file_idxs[i].strip().split(' ')[0])\n",
    "        #print(file_idxs[i])        \n",
    "        #f.write(str(file_idxs[i])+'\\n')\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Observations**\n",
    "\n",
    "> We find that frequency component 8 corresponding to 7000-8000 Hz, seems to get maximally activated for making the prediction. Still, its hard to make any signifant conclusion about a class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick 5 audio files for hearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random_5_ids = [571,455,387,202,708]   # Genuine True positive, >90% probability\n",
    "#add 1 to each to access correct file index in real world\n",
    "\n",
    "#random_5_ids = [572,456,388,203,709]   # Genuine True positive, >90% probability\n",
    "#base='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#cp /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_dev/D_1000709.wav audio_files_hearing/genuine_TP/\n",
    "\n",
    "#play audio_files_hearing/genuine_TP/D_1000507.wav\n",
    "#ls audio_files_hearing/genuine_TP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal and spectral segementation\n",
    "\n",
    "Our input to the CNN is a magnitude spectrogram of duration 3 seconds. We partition the input into ten uniform components, each of 300 miliseconds to generate temporal explanation. Similary, along the frequency axis, we partition the spectrogram into eight different spectral components. Each component corresponds to about 1333 Hz except the last component that represents information between ABC to XYZ Hz.\n",
    "\n",
    "\n",
    "# On Development set\n",
    "\n",
    "The development set has 760 genuine audio files and 950 spoofed examples. \n",
    "\n",
    "\n",
    "## Explaining predictions for genuine class : what CNN thinks about a Genuine Class ?\n",
    "\n",
    "We first collect all the genuine audio files that has been strongly correctly classified with more than 90% probability by the CNN model. We find 685 audio files (out of 760). Next, we run SLIME algorithm on these 685 audio files and generate temporal and spectral explanations.\n",
    "\n",
    "Distribution of **temporal explanation** for \n",
    "\n",
    "    Component 0 : 18\n",
    "    Component 1 : 0\n",
    "    Component 2 : 112\n",
    "    Component 3 : 247\n",
    "    Component 4 : 31\n",
    "    Component 5 : 160\n",
    "    Component 6 : 95\n",
    "    Component 7 : 21\n",
    "    Component 8 : 1\n",
    "    Component 9 : 0\n",
    "    \n",
    "Distribution of **Spectral explanation**\n",
    "\n",
    "    Component 0 : 2\n",
    "    Component 1 : 0\n",
    "    Component 2 : 0\n",
    "    Component 3 : 0\n",
    "    Component 4 : 0\n",
    "    Component 5 : 9\n",
    "    Component 6 : 0\n",
    "    Component 7 : 674\n",
    "\n",
    "    \n",
    "    \n",
    "> How do we represent the above distribution? Table or bar-chart or what? First we need to be showing this on the training data to claim what the CNN has learned from the training data. Then we move on to prove the hypothesis on the development and the evaluation data.\n",
    "\n",
    "    \n",
    "On average, the explanation distribution show strong emphasis in the center of the 3 second audio signal. This further corresponds the region in audio signal where speech onset is detected. We find that majority of genuine audio files in the development data has non-speech frames in the beginning. On the spectral explanation distribution, it gives a very clear explanation that the model is looking at high frequency information above 7 kHz. \n",
    "\n",
    "> Therefore, what **conclusion we get from above** is that th model is identifying a file as genuine using information above 7 kHz mostly in the middle of the audio signal.\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18+112+247+31+160+95+21+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "667"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "685-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
