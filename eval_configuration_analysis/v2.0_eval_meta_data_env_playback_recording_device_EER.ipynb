{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ASVspoof 2017 v2.0 evaluation dataset meta-data analysis\n",
    "\n",
    "> Color code specifies the level of difficulty\n",
    " 1. **Green** indicates the easiest case to be detected by the system\n",
    " 1. **Yellow** - medium level\n",
    " 1. **Red** - difficult level\n",
    " \n",
    " \n",
    "Using the following end-to-end CNN model (the one we submitted in INTERSPEECH), we compute EER for different qualities of environment, playback and the recording devices\n",
    "\n",
    "> **models_After_ICASSP/InterSpeech2018_v2.0/testing_best_model_with3sec_RELU/keep_0.5_0.5_relurun8 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment ID List :\n",
    "\n",
    "1. Red color - the hardest ones (Total = 5)\n",
    "\n",
    "['E01','E23','E24','E25','E26']\n",
    "\n",
    "2. Green color - the easiest to detect as it is the noisy ones: total = 3\n",
    "\n",
    "['E02','E03','E06']\n",
    "\n",
    "3. Yello color - the medium : total = 18\n",
    "\n",
    "['E04','E05','E07','E08','E09','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yellowE = ['E04','E05','E07','E08','E09','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "greenE = ['E02','E03','E06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redE = ['E01','E23','E24','E25','E26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(yellowE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playback devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Red color, total = 12\n",
    "\n",
    "['P03','P04','P05','P07','P09','P13','P15','P22','P23','P24','P25','P26']\n",
    "\n",
    "> Yellow color, total = 5\n",
    "\n",
    "['P01','P02','P12','P14','P19']\n",
    "\n",
    "> Green color, total = 9\n",
    "\n",
    "['P06','P08','P10','P11','P16','P17','P18','P20','P21']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifiying the counts\n",
    "\n",
    "> The above counts obtained for various devices and environment configuration has been verified with the baseline paper by hector. It matches perfect.\n",
    "\n",
    "> It would be better to also create an scp and protocal list for each of them so in case, if we want to perform analysis for them !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redP = ['P03','P04','P05','P07','P09','P13','P15','P22','P23','P24','P25','P26']\n",
    "yellowP = ['P01','P02','P12','P14','P19']\n",
    "greenP = ['P06','P08','P10','P11','P16','P17','P18','P20','P21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "5\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(redP))\n",
    "print(len(yellowP))\n",
    "print(len(greenP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording device\n",
    "\n",
    "> Red color, total = 13\n",
    "\n",
    "['R01','R05','R06','R08','R09','R10','R11','R16','R21','R22','R23','R24','R25']\n",
    "\n",
    "\n",
    "> Yello color, total = 02\n",
    "\n",
    "['R03','R15']\n",
    "\n",
    "> Green color, total = 10\n",
    "\n",
    "['R02','R04','R07','R12','R13','R14','R17','R18','R19','R20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redR = ['R01','R05','R06','R08','R09','R10','R11','R16','R21','R22','R23','R24','R25']\n",
    "yellowR = ['R03','R15']\n",
    "greenR = ['R02','R04','R07','R12','R13','R14','R17','R18','R19','R20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "2\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(redR))\n",
    "print(len(yellowR))\n",
    "print(len(greenR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay configurations in the evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation dataset has total of 57 replay configurations. Please refer to the Hector odyssey paper to see more details on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evalProt = '/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "protList = []\n",
    "with open(evalProt) as f:\n",
    "    for line in f:\n",
    "        rc = line.strip().split(' ')[4] + ' '+  line.strip().split(' ')[5] + ' ' + line.strip().split(' ')[6]\n",
    "        if rc not in protList and rc != '- - -':\n",
    "            protList.append(rc)\n",
    "    #print(protList)\n",
    "    print(len(protList))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** The eval set has 57 new spoofing configurations ! (verified with the paper )**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Inferring RC-specific file list from the score files\n",
    "\n",
    "> Note that in most of our implementation we used genuineFirstSpoof scp and label list for the evaluation data. Here, we first extracted all the genuine trials in the beginngin and put all the spoof later (like the way trials are organised in the training and the validation sets). There is an immediate need to clean and organize the code structure with proper documentation so that few months later when you read the code you do not get confused on the implementation. Make it a bit organised !\n",
    "\n",
    "> However, we did not create the corresponding protocal file having all the meta-data information for these files. so we will have to carefully infer these meta-data for the score files.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the scp file where we put all genuine files first then spoof\n",
    "\n",
    "evalScp='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/filelists/eval_genFirstSpoof.scp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The original protocal file where genuine and spoof are mixed up\n",
    "\n",
    "evalProt='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The model prediction scores on eval data that use evalScp for spectrogram computation\n",
    "# one example\n",
    "\n",
    "model='/homes/bc305/myphd/stage2/deeplearning.experiment1/CNN3//models_After_ICASSP/InterSpeech2018_v2.0/testing_best_model_with3sec_RELU/'\n",
    "evalScore=model+'/keep_0.5_0.5_relurun8/predictions_original/eval_prediction_new.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With above function we generated a new protocal file for the evaluation data that is based on the genuineFirst\n",
    " spoof next criterion we used during spectrogram computation etc\n",
    " \n",
    "> **new protocal** /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl_genFirstSpoof.txt  \n",
    "\n",
    "> The code for producing the new protocal is here: python_codes/make_genuineFirstspoof_protocal_evalset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_1000010.wav genuine M0035 S06 - - -\n",
      "E_1000018.wav genuine M0023 S01 - - -\n",
      "E_1000074.wav genuine M0031 S09 - - -\n",
      "E_1000102.wav genuine M0030 S09 - - -\n",
      "E_1000123.wav genuine M0029 S10 - - -\n",
      "E_1000151.wav genuine M0028 S07 - - -\n",
      "E_1000161.wav genuine M0034 S10 - - -\n",
      "E_1000179.wav genuine M0025 S05 - - -\n",
      "E_1000192.wav genuine M0029 S08 - - -\n",
      "E_1000217.wav genuine M0028 S05 - - -\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl_genFirstSpoof.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE 1: Environment specific list\n",
    "\n",
    "> **redE** means the hardest one to detect. Please refer to the baseline paper for more details\n",
    "\n",
    "> **yellowE** the medium hard and **green** the easiest ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from eer import find_eers\n",
    "from python_codes import analyse_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreFile = evalScore\n",
    "evalScores = analyse_conf.get_scores(scoreFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evProtocal='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl_genFirstSpoof.txt'\n",
    "evalScpFile='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/filelists/eval_genFirstSpoof.scp'\n",
    "\n",
    "with open(evalScpFile) as f:\n",
    "    evalScp = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)  for greenE ['E02', 'E03', 'E06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1939\n",
      "1939\n"
     ]
    }
   ],
   "source": [
    "# E02\n",
    "\n",
    "confKey = ['E02']\n",
    "saveFolder = 'E02'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   38.7762\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/E02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411\n",
      "1411\n"
     ]
    }
   ],
   "source": [
    "# E03\n",
    "\n",
    "confKey = ['E03']\n",
    "saveFolder = 'E03'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   32.5645\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/E03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583\n",
      "1583\n"
     ]
    }
   ],
   "source": [
    "# E06\n",
    "\n",
    "confKey = ['E06']\n",
    "saveFolder = 'E06'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "    3.3726\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/E06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2337\n",
      "2337\n"
     ]
    }
   ],
   "source": [
    "# Lets do for greenE, the easiest one first: all three in one: E02_E03_E06\n",
    "\n",
    "confKey = ['E02','E03','E06']\n",
    "gc=1298\n",
    "saveFolder = 'greenEnvironment'\n",
    "\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   32.7170\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/greenEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EER stats on greenE, the easiest ones as reported in baseline\n",
    "\n",
    "Using the scores that was produced by our end-to-end model (we reported in Interspeech analysis paper), that showed about 5% EER on DEV and about 29.4 % on the eval set. We computed the EER with respect to the green environment (that is highly noisly and should have been easily detected by the system - thats what the baseline paper argues which use the CQCC features. But our system perform worse still, the reason is fairly simply if we corerelate our INTERSPEECH findings about the classes. )\n",
    "\n",
    "> **E02** has 641 spoof (+1298 genuine), EER = 38.77\n",
    "\n",
    "> **E03** has 113 spoof (+1298 genuine), EER = 32.56\n",
    "\n",
    "> **E06** has 285 spoof (+1298 genuine), EER = 3.3726\n",
    "\n",
    "> **greenEnvironment (E02,E03,E06) ** has 1039 spoof + 1298 genuine), EER = 32.71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii)  for redE = ['E01','E23','E24','E25','E26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2931\n",
      "2931\n"
     ]
    }
   ],
   "source": [
    "# Lets do for redE, the most difficult ones as characterized in the paper\n",
    "\n",
    "confKey = ['E01','E23','E24','E25','E26']\n",
    "gc=1298\n",
    "saveFolder = 'redEnvironment'\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   19.3090\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/redEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Just to double check number of files under redE\n",
    "\n",
    "\n",
    "cat '/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl.txt' | grep E26 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2931"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "748+342+183+182+178+1298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EER stats on redE, the most difficult ones\n",
    "\n",
    "> redE = ['E01','E23','E24','E25','E26']\n",
    "\n",
    "> **redEnvironment ** has total 2931 files (out of which first 1298 are genuine), EER = 19.3. This is very interesting as our system in this configuration seems to ***give better results than the baseline (21.86).*** See Hectors paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii)  for yellowE = ['E04','E05','E07','E08','E09','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22'] \n",
    "\n",
    "The medium difficulty level environment !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10634\n",
      "10634\n"
     ]
    }
   ],
   "source": [
    "# Lets do for redE, the most difficult ones as characterized in the paper\n",
    "\n",
    "confKey = yellowE   # see details of yellowE above\n",
    "gc=1298\n",
    "saveFolder='yellowEnvironment'\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   30.7771\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/yellowEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation on environment specific results\n",
    "\n",
    "> **greenEnvironment (E02,E03,E06) ** has 1039 spoof + 1298 genuine), EER = 32.71\n",
    "\n",
    "> **yellowEnvironment ** has total 10634 files (out of which first 1298 are genuine), EER = 30.77. May be these contain those files that we found earlier in our analysis work that bear genuine file characteristics. Need to double check though.\n",
    "\n",
    "> **redEnvironment ** has total 2931 files (out of which first 1298 are genuine), EER = 19.3. This is very interesting as our system in this configuration seems to ***give better results than the baseline (21.86).*** See Hectors paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1633"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2931-1298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE 2: Playback device specific list\n",
    "\n",
    "> **redE** means the hardest one to detect. Please refer to the baseline paper for more details\n",
    "\n",
    "> **yellowE** the medium hard and **green** the easiest ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)  for greenP ['P06', 'P08', 'P10', 'P11', 'P16', 'P17', 'P18', 'P20', 'P21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P06', 'P08', 'P10', 'P11', 'P16', 'P17', 'P18', 'P20', 'P21']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greenP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5910\n",
      "5910\n"
     ]
    }
   ],
   "source": [
    "# Lets do for greenP, the easiest one first\n",
    "\n",
    "confKey = greenP                  # pass this as a list\n",
    "saveFolder='greenPlayback'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   42.9804\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/greenPlayback/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii)  for yellowP ['P01', 'P02', 'P12', 'P14', 'P19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P01', 'P02', 'P12', 'P14', 'P19']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellowP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2866\n",
      "2866\n"
     ]
    }
   ],
   "source": [
    "confKey = yellowP                  # pass this as a list\n",
    "saveFolder='yellowPlayback'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   22.4272\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/yellowPlayback/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii)  for redP  ['P03', 'P04', 'P05', 'P07', 'P09', 'P13', 'P15', 'P22', 'P23', 'P24', 'P25', 'P26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P03',\n",
       " 'P04',\n",
       " 'P05',\n",
       " 'P07',\n",
       " 'P09',\n",
       " 'P13',\n",
       " 'P15',\n",
       " 'P22',\n",
       " 'P23',\n",
       " 'P24',\n",
       " 'P25',\n",
       " 'P26']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7126\n",
      "7126\n"
     ]
    }
   ],
   "source": [
    "confKey = redP                  # pass this as a list\n",
    "saveFolder='redPlayback'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   17.4575\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/redPlayback/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations on Playback devices**\n",
    "\n",
    "> **greenP** has 5910 total files (first 1298 are genuine and remaining spoof). EER = 42.98\n",
    "\n",
    "> **yellowP** has 2866 total files (first 1298 are genuine and remaining spoof). EER = 22.42\n",
    "\n",
    "> **redP** has 7126 total files (first 1298 are genuine and remaining spoof). EER = 17.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5828"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7126-1298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE 3: Recording device specific list\n",
    "\n",
    "> **redE** means the hardest one to detect. Please refer to the baseline paper for more details\n",
    "\n",
    "> **yellowE** the medium hard and **green** the easiest ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)  for greenR: ['R02', 'R04', 'R07', 'R12', 'R13', 'R14', 'R17', 'R18', 'R19', 'R20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R02', 'R04', 'R07', 'R12', 'R13', 'R14', 'R17', 'R18', 'R19', 'R20']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greenR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6390\n",
      "6390\n"
     ]
    }
   ],
   "source": [
    "confKey = greenR                  # pass this as a list\n",
    "saveFolder='greenRecording'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5092"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6390-1298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   33.0372\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/greenRecording/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii)  for yellowR: ['R03', 'R15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R03', 'R15']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellowR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2890\n",
      "2890\n"
     ]
    }
   ],
   "source": [
    "confKey = yellowR                  # pass this as a list\n",
    "saveFolder='yellowRecording'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   31.9441\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/yellowRecording/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii)  for redR: ['R01', 'R05', 'R06', 'R08', 'R09', 'R10', 'R11', 'R16', 'R21', 'R22', 'R23', 'R24', 'R25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R01',\n",
       " 'R05',\n",
       " 'R06',\n",
       " 'R08',\n",
       " 'R09',\n",
       " 'R10',\n",
       " 'R11',\n",
       " 'R16',\n",
       " 'R21',\n",
       " 'R22',\n",
       " 'R23',\n",
       " 'R24',\n",
       " 'R25']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6622\n",
      "6622\n"
     ]
    }
   ],
   "source": [
    "confKey = redR                  # pass this as a list\n",
    "saveFolder='redRecording'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   24.2499\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/redRecording/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations on Recording devices**\n",
    "\n",
    "> **greenR** has 6390 total files (first 1298 are genuine and remaining spoof). EER = 33.03\n",
    "\n",
    "> **yellowR** has 2890 total files (first 1298 are genuine and remaining spoof). EER = 31.9\n",
    "\n",
    "> **redR** has 6622 total files (first 1298 are genuine and remaining spoof). EER = 24.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5324"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6622-1298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input: a) pass the score file and the new protocal filelist corresponding to these scores\n",
    "#        b) search configuration (could be environment, playback etc)\n",
    "\n",
    "# Output:\n",
    "#        a) configuration specific score files along with labels\n",
    "#        b) should i write this in a file or find a way to pass this into EER computing script\n",
    "#        c) return the EER for this configuration\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_config_specific_scores11(scoreFile, protocal, scpFile, confKeys):\n",
    "    \n",
    "    '''\n",
    "    This function returns the scores corresponding to replay config parameter in confkey\n",
    "    Inputs\n",
    "       1) scoreFile: this is a score file as per genuineFirstspoof list which is often used in my work\n",
    "       2) protocal:  denotes the genuineFirstspoof protocal file we created using genuineFirstSpoof.scp file\n",
    "       2) scpFile :  this is the scp file that holds the path to the audio files.\n",
    "       3) confKeys : could be a list of configuration key based on which we want to search the spoof scores.\n",
    "                     This is basically used to create sub-score list using config and evaluate the EER\n",
    "                     \n",
    "    NOTE: if you use it for dev and train set, you need to be careful passing correct files !                 \n",
    "                     \n",
    "    Output:\n",
    "       1) list of scores that match keys in the confKeys list\n",
    "       2) list of scps that match keys in the confKeys list. This is to create filepath for all those spoof files\n",
    "          This will be useful for analysing later, may be using slime !\n",
    "                     \n",
    "    '''\n",
    "    \n",
    "    # Read the scp file into a list\n",
    "    with open(scpFile) as f:\n",
    "        scps = [line.strip() for line in f]\n",
    "                \n",
    "    # Read scores and put into a list\n",
    "    with open(scoreFile) as f:\n",
    "        scores = [line.strip() for line in f]\n",
    "        \n",
    "    # Read protocal files into a list\n",
    "    with open(protocal) as f:\n",
    "        prots = [line.strip() for line in f]  \n",
    "    \n",
    "   \n",
    "    # Loop over all the items/keys in the confkey and append all the corresponding scores    \n",
    "    count=0\n",
    "    confKey_scores = list()\n",
    "    confKey_scps = list()\n",
    "    \n",
    "    for confKey in confKeys:\n",
    "        # find all those files that match confKey in the protocal, and get the respective scores    \n",
    "        \n",
    "        for i in range(0,len(prots)):\n",
    "            if confKey in prots[i]:            \n",
    "                # append the spoof scores that match the confKey\n",
    "                confKey_scores.append(scores[i])\n",
    "                confKey_scps.append(scps[i])\n",
    "                \n",
    "                count += 1\n",
    "        \n",
    "    #print('Total spoof in this config is: ', count)    \n",
    "    \n",
    "    return confKey_scores, confKey_scps\n",
    "    # Just return all the scores and scps that match confKey\n",
    "    \n",
    "    \n",
    "def save_scores_labels_scps(savePath,scores,labels,scps,confKey):\n",
    "    '''\n",
    "    This function is used to save the scores and labels as per confkeys\n",
    "    \n",
    "    Inputs\n",
    "         1) savePath: where to save the scores? \n",
    "         2) scores and labels are corresponding to the confKey\n",
    "         3) scps: which will have first genuine and then all those spoof ones that matched confKey\n",
    "    \n",
    "    '''\n",
    "              \n",
    "    #First create directories related to confKey for saving scores and labels\n",
    "    saveDir = savePath+'/'+confKey\n",
    "            \n",
    "    make_directory(saveDir)    \n",
    "        \n",
    "    with open(saveDir+'/score.txt', 'w') as f:\n",
    "        for s in scores:\n",
    "            f.write(str(s)+'\\n')\n",
    "\n",
    "    with open(saveDir+'/labels.txt','w') as f:\n",
    "        for l in labels:\n",
    "            if l == 1.0:\n",
    "                out='genuine'\n",
    "            else:\n",
    "                out='spoof'\n",
    "                \n",
    "            f.write(out+'\\n')   \n",
    "            \n",
    "    with open(saveDir+'audio.scp','w') as f:\n",
    "        for path in scps:\n",
    "            f.write(path+'\\n')\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
