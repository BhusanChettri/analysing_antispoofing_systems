{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ASVspoof 2017 v2.0 evaluation dataset meta-data analysis\n",
    "\n",
    "> Color code specifies the level of difficulty\n",
    " 1. **Green** indicates the easiest case to be detected by the system\n",
    " 1. **Yellow** - medium level\n",
    " 1. **Red** - difficult level\n",
    " \n",
    " \n",
    "Using the following end-to-end CNN model (the one we submitted in INTERSPEECH), we compute EER for different qualities of environment, playback and the recording devices\n",
    "\n",
    "> models_After_ICASSP/InterSpeech2018_v2.0/testing_best_model_with3sec_RELU/keep_0.5_0.5_relurun8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment ID List :\n",
    "\n",
    "1. Red color - the hardest ones (Total = 5)\n",
    "\n",
    "['E01','E23','E24','E25','E26']\n",
    "\n",
    "2. Green color - the easiest to detect as it is the noisy ones: total = 3\n",
    "\n",
    "['E02','E03','E06']\n",
    "\n",
    "3. Yello color - the medium : total = 18\n",
    "\n",
    "['E04','E05','E07','E08','E09','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yellowE = ['E04','E05','E07','E08','E09','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "greenE = ['E02','E03','E06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redE = ['E01','E23','E24','E25','E26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(yellowE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playback devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Red color, total = 12\n",
    "\n",
    "['P03','P04','P05','P07','P09','P13','P15','P22','P23','P24','P25','P26']\n",
    "\n",
    "> Yellow color, total = 5\n",
    "\n",
    "['P01','P02','P12','P14','P19']\n",
    "\n",
    "> Green color, total = 9\n",
    "\n",
    "['P06','P08','P10','P11','P16','P17','P18','P20','P21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redP = ['P03','P04','P05','P07','P09','P13','P15','P22','P23','P24','P25','P26']\n",
    "yellowP = ['P01','P02','P12','P14','P19']\n",
    "greenP = ['P06','P08','P10','P11','P16','P17','P18','P20','P21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "5\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(redP))\n",
    "print(len(yellowP))\n",
    "print(len(greenP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording device\n",
    "\n",
    "> Red color, total = 13\n",
    "\n",
    "['R01','R05','R06','R08','R09','R10','R11','R16','R21','R22','R23','R24','R25']\n",
    "\n",
    "\n",
    "> Yello color, total = 02\n",
    "\n",
    "['R03','R15']\n",
    "\n",
    "> Green color, total = 10\n",
    "\n",
    "['R02','R04','R07','R12','R13','R14','R17','R18','R19','R20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redR = ['R01','R05','R06','R08','R09','R10','R11','R16','R21','R22','R23','R24','R25']\n",
    "yellowR = ['R03','R15']\n",
    "greenR = ['R02','R04','R07','R12','R13','R14','R17','R18','R19','R20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "2\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(redR))\n",
    "print(len(yellowR))\n",
    "print(len(greenR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay configurations in the evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation dataset has total of 57 replay configurations. Please refer to the Hector odyssey paper to see more details on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evalProt = '/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "protList = []\n",
    "with open(evalProt) as f:\n",
    "    for line in f:\n",
    "        rc = line.strip().split(' ')[4] + ' '+  line.strip().split(' ')[5] + ' ' + line.strip().split(' ')[6]\n",
    "        if rc not in protList and rc != '- - -':\n",
    "            protList.append(rc)\n",
    "    #print(protList)\n",
    "    print(len(protList))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Inferring RC-specific file list from the score files\n",
    "\n",
    "> Note that in most of our implementation we used genuineFirstSpoof scp and label list for the evaluation data. Here, we first extracted all the genuine trials in the beginngin and put all the spoof later (like the way trials are organised in the training and the validation sets). There is an immediate need to clean and organize the code structure with proper documentation so that few months later when you read the code you do not get confused on the implementation. Make it a bit organised !\n",
    "\n",
    "> However, we did not create the corresponding protocal file having all the meta-data information for these files. so we will have to carefully infer these meta-data for the score files.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the scp file where we put all genuine files first then spoof\n",
    "\n",
    "evalScp='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/filelists/eval_genFirstSpoof.scp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The original protocal file where genuine and spoof are mixed up\n",
    "\n",
    "evalProt='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The model prediction scores on eval data that use evalScp for spectrogram computation\n",
    "# one example\n",
    "\n",
    "model='/homes/bc305/myphd/stage2/deeplearning.experiment1/CNN3//models_After_ICASSP/InterSpeech2018_v2.0/testing_best_model_with3sec_RELU/'\n",
    "evalScore=model+'/keep_0.5_0.5_relurun8/predictions_original/eval_prediction_new.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With above function we generated a new protocal file for the evaluation data that is based on the genuineFirst\n",
    " spoof next criterion we used during spectrogram computation etc\n",
    " \n",
    "> **new protocal** /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl_genFirstSpoof.txt  \n",
    "\n",
    "> The code for producing the new protocal is here: python_codes/make_genuineFirstspoof_protocal_evalset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#cat /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl_genFirstSpoof.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE 1: Environment specific list\n",
    "\n",
    "> **redE** means the hardest one to detect. Please refer to the baseline paper for more details\n",
    "\n",
    "> **yellowE** the medium hard and **green** the easiest ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from eer import find_eers\n",
    "from python_codes import analyse_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreFile = evalScore\n",
    "evalScores = analyse_conf.get_scores(scoreFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evProtocal='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl_genFirstSpoof.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)  for greenE ['E02', 'E03', 'E06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1939\n"
     ]
    }
   ],
   "source": [
    "# Lets do for greenE, the easiest one first\n",
    "#['E02', 'E03', 'E06']\n",
    "\n",
    "confKey = ['E02']\n",
    "saveFolder = 'E02'\n",
    "gc=1298\n",
    "\n",
    "E02_scores_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, confKey)\n",
    "E02_scores_gen = evalScores[0:gc]\n",
    "E02_scores = np.hstack((E02_scores_gen, E02_scores_spf))\n",
    "\n",
    "E02_labels = np.hstack((np.ones(len(E02_scores_gen)), np.zeros(len(E02_scores_spf))))\n",
    "\n",
    "print(len(E02_scores))\n",
    "\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels(scoreSavePath, E02_scores,E02_labels,saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "EER =\n",
      "\n",
      "   38.7762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/E02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411\n"
     ]
    }
   ],
   "source": [
    "confKey = ['E03']\n",
    "gc=1298\n",
    "saveFolder = 'E03'\n",
    "\n",
    "E03_scores_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, confKey)\n",
    "E03_scores_gen = evalScores[0:gc]\n",
    "E03_scores = np.hstack((E03_scores_gen, E03_scores_spf))\n",
    "\n",
    "E03_labels = np.hstack((np.ones(len(E03_scores_gen)), np.zeros(len(E03_scores_spf))))\n",
    "\n",
    "print(len(E03_scores))\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels(scoreSavePath, E03_scores,E03_labels,saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "EER =\n",
      "\n",
      "   32.5645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/E03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583\n"
     ]
    }
   ],
   "source": [
    "# Lets do for greenE, the easiest one first\n",
    "#['E06', 'E06', 'E06']\n",
    "\n",
    "confKey = ['E06']\n",
    "gc=1298\n",
    "saveFolder = 'E06'\n",
    "\n",
    "E06_scores_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, confKey)\n",
    "E06_scores_gen = evalScores[0:gc]\n",
    "E06_scores = np.hstack((E06_scores_gen, E06_scores_spf))\n",
    "\n",
    "E06_labels = np.hstack((np.ones(len(E06_scores_gen)), np.zeros(len(E06_scores_spf))))\n",
    "\n",
    "print(len(E06_scores))\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels(scoreSavePath, E06_scores,E06_labels,saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "EER =\n",
      "\n",
      "    3.3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/E06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2337\n"
     ]
    }
   ],
   "source": [
    "# Lets do for greenE, the easiest one first: all three in one: E02_E03_E06\n",
    "\n",
    "confKey = ['E02','E03','E06']\n",
    "gc=1298\n",
    "saveFolder = 'greenEnvironment'\n",
    "\n",
    "# Concatenate all spoof scores\n",
    "E02_E03_E06_scores_spf =  analyse_conf.get_config_specific_scores(scoreFile, evProtocal, confKey)\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "# Put genuine scores on top\n",
    "scores = np.hstack((scores_gen, E02_E03_E06_scores_spf))\n",
    "\n",
    "# Make the labels\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(E02_E03_E06_scores_spf))))\n",
    "\n",
    "print(len(scores))\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels(scoreSavePath, scores, labels, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "EER =\n",
      "\n",
      "   32.7170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/greenEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EER stats on greenE, the easiest ones as reported in baseline\n",
    "\n",
    "Using the scores that was produced by our end-to-end model (we reported in Interspeech analysis paper), that showed about 5% EER on DEV and about 29.4 % on the eval set. We computed the EER with respect to the green environment (that is highly noisly and should have been easily detected by the system - thats what the baseline paper argues which use the CQCC features. But our system perform worse still, the reason is fairly simply if we corerelate our INTERSPEECH findings about the classes. )\n",
    "\n",
    "> **E02** has 641 spoof (+1298 genuine), EER = 38.77\n",
    "\n",
    "> **E03** has 113 spoof (+1298 genuine), EER = 32.56\n",
    "\n",
    "> **E06** has 285 spoof (+1298 genuine), EER = 3.3726\n",
    "\n",
    "> **greenEnvironment (E02,E03,E06) ** has 1039 spoof + 1298 genuine), EER = 32.71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii)  for redE = ['E01','E23','E24','E25','E26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2931\n"
     ]
    }
   ],
   "source": [
    "# Lets do for redE, the most difficult ones as characterized in the paper\n",
    "\n",
    "confKey = ['E01','E23','E24','E25','E26']\n",
    "gc=1298\n",
    "saveFolder = 'redEnvironment'\n",
    "\n",
    "# Concatenate all spoof scores\n",
    "red_scores_spf =  analyse_conf.get_config_specific_scores(scoreFile, evProtocal, confKey)\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "# Put genuine scores on top\n",
    "scores = np.hstack((scores_gen, red_scores_spf))\n",
    "\n",
    "# Make the labels\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(red_scores_spf))))\n",
    "\n",
    "print(len(scores))\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels(scoreSavePath, scores, labels, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "EER =\n",
      "\n",
      "   19.3090\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/redEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Just to double check number of files under redE\n",
    "\n",
    "\n",
    "cat '/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl.txt' | grep E26 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2931"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "748+342+183+182+178+1298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EER stats on redE, the most difficult ones\n",
    "\n",
    "> redE = ['E01','E23','E24','E25','E26']\n",
    "\n",
    "> **redEnvironment ** has total 2931 files (out of which first 1298 are genuine), EER = 19.3. This is very interesting as our system in this configuration seems to ***give better results than the baseline (21.86).*** See Hectors paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii)  for yellowE = ['E04','E05','E07','E08','E09','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22'] \n",
    "\n",
    "The medium difficulty level environment !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10634\n"
     ]
    }
   ],
   "source": [
    "# Lets do for redE, the most difficult ones as characterized in the paper\n",
    "\n",
    "confKey = yellowE   # see details of yellowE above\n",
    "gc=1298\n",
    "saveFolder='yellowEnvironment'\n",
    "\n",
    "# Concatenate all spoof scores\n",
    "yellow_scores_spf =  analyse_conf.get_config_specific_scores(scoreFile, evProtocal, confKey)\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "# Put genuine scores on top\n",
    "scores = np.hstack((scores_gen, yellow_scores_spf))\n",
    "\n",
    "# Make the labels\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(yellow_scores_spf))))\n",
    "\n",
    "print(len(scores))\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels(scoreSavePath, scores, labels, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "EER =\n",
      "\n",
      "   30.7771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/yellowEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observaation on environment specific results\n",
    "\n",
    "> **greenEnvironment (E02,E03,E06) ** has 1039 spoof + 1298 genuine), EER = 32.71\n",
    "\n",
    "> **yellowEnvironment ** has total 10634 files (out of which first 1298 are genuine), EER = 30.77. May be these contain those files that we found earlier in our analysis work that bear genuine file characteristics. Need to double check though.\n",
    "\n",
    "> **redEnvironment ** has total 2931 files (out of which first 1298 are genuine), EER = 19.3. This is very interesting as our system in this configuration seems to ***give better results than the baseline (21.86).*** See Hectors paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1633"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2931-1298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE 2: Playback device specific list\n",
    "\n",
    "> **redE** means the hardest one to detect. Please refer to the baseline paper for more details\n",
    "\n",
    "> **yellowE** the medium hard and **green** the easiest ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)  for greenP ['P06', 'P08', 'P10', 'P11', 'P16', 'P17', 'P18', 'P20', 'P21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P06', 'P08', 'P10', 'P11', 'P16', 'P17', 'P18', 'P20', 'P21']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greenP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5910\n"
     ]
    }
   ],
   "source": [
    "# Lets do for greenP, the easiest one first\n",
    "\n",
    "confKey = greenP                  # pass this as a list\n",
    "saveFolder='greenPlayback'\n",
    "gc=1298\n",
    "\n",
    "# Concatenate all spoof scores\n",
    "green_scores_spf =  analyse_conf.get_config_specific_scores(scoreFile, evProtocal, confKey)\n",
    "\n",
    "scores_gen = evalScores[0:gc]   # get genuine scores from eval scores\n",
    "\n",
    "# Put genuine scores on top\n",
    "scores = np.hstack((scores_gen, green_scores_spf))\n",
    "\n",
    "# Make the labels\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(green_scores_spf))))\n",
    "\n",
    "print(len(scores))\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels(scoreSavePath, scores, labels, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "EER =\n",
      "\n",
      "   42.9804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/greenPlayback/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii)  for yellowP ['P01', 'P02', 'P12', 'P14', 'P19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P01', 'P02', 'P12', 'P14', 'P19']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellowP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2866\n"
     ]
    }
   ],
   "source": [
    "confKey = yellowP                  # pass this as a list\n",
    "saveFolder='yellowPlayback'\n",
    "gc=1298\n",
    "\n",
    "# Concatenate all spoof scores\n",
    "green_scores_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, confKey)\n",
    "\n",
    "scores_gen = evalScores[0:gc]   # get genuine scores from eval scores\n",
    "\n",
    "# Put genuine scores on top\n",
    "scores = np.hstack((scores_gen, green_scores_spf))\n",
    "\n",
    "# Make the labels\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(green_scores_spf))))\n",
    "\n",
    "print(len(scores))\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels(scoreSavePath, scores, labels, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "EER =\n",
      "\n",
      "   22.4272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/yellowPlayback/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii)  for redP  ['P03', 'P04', 'P05', 'P07', 'P09', 'P13', 'P15', 'P22', 'P23', 'P24', 'P25', 'P26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P03',\n",
       " 'P04',\n",
       " 'P05',\n",
       " 'P07',\n",
       " 'P09',\n",
       " 'P13',\n",
       " 'P15',\n",
       " 'P22',\n",
       " 'P23',\n",
       " 'P24',\n",
       " 'P25',\n",
       " 'P26']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7126\n"
     ]
    }
   ],
   "source": [
    "confKey = redP                  # pass this as a list\n",
    "saveFolder='redPlayback'\n",
    "gc=1298\n",
    "\n",
    "# Concatenate all spoof scores\n",
    "green_scores_spf =  analyse_conf.get_config_specific_scores(scoreFile, evProtocal, confKey)\n",
    "\n",
    "scores_gen = evalScores[0:gc]   # get genuine scores from eval scores\n",
    "\n",
    "# Put genuine scores on top\n",
    "scores = np.hstack((scores_gen, green_scores_spf))\n",
    "\n",
    "# Make the labels\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(green_scores_spf))))\n",
    "\n",
    "print(len(scores))\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels(scoreSavePath, scores, labels, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "EER =\n",
      "\n",
      "   17.4575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/redPlayback/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations on Playback devices**\n",
    "\n",
    "> **greenP** has 5910 total files (first 1298 are genuine and remaining spoof). EER = 42.98\n",
    "\n",
    "> **yellowP** has 2866 total files (first 1298 are genuine and remaining spoof). EER = 22.42\n",
    "\n",
    "> **redP** has 7126 total files (first 1298 are genuine and remaining spoof). EER = 17.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5828"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7126-1298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE 3: Recording device specific list\n",
    "\n",
    "> **redE** means the hardest one to detect. Please refer to the baseline paper for more details\n",
    "\n",
    "> **yellowE** the medium hard and **green** the easiest ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)  for greenR: ['R02', 'R04', 'R07', 'R12', 'R13', 'R14', 'R17', 'R18', 'R19', 'R20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R02', 'R04', 'R07', 'R12', 'R13', 'R14', 'R17', 'R18', 'R19', 'R20']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greenR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6390\n"
     ]
    }
   ],
   "source": [
    "confKey = greenR                  # pass this as a list\n",
    "saveFolder='greenRecording'\n",
    "gc=1298\n",
    "\n",
    "# Concatenate all spoof scores\n",
    "green_scores_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, confKey)\n",
    "\n",
    "scores_gen = evalScores[0:gc]   # get genuine scores from eval scores\n",
    "\n",
    "# Put genuine scores on top\n",
    "scores = np.hstack((scores_gen, green_scores_spf))\n",
    "\n",
    "# Make the labels\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(green_scores_spf))))\n",
    "\n",
    "print(len(scores))\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels(scoreSavePath, scores, labels, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5092"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6390-1298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "EER =\n",
      "\n",
      "   33.0372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/greenRecording/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii)  for yellowR: ['R03', 'R15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R03', 'R15']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellowR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2890\n"
     ]
    }
   ],
   "source": [
    "confKey = yellowR                  # pass this as a list\n",
    "saveFolder='yellowRecording'\n",
    "gc=1298\n",
    "\n",
    "# Concatenate all spoof scores\n",
    "green_scores_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, confKey)\n",
    "\n",
    "scores_gen = evalScores[0:gc]   # get genuine scores from eval scores\n",
    "\n",
    "# Put genuine scores on top\n",
    "scores = np.hstack((scores_gen, green_scores_spf))\n",
    "\n",
    "# Make the labels\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(green_scores_spf))))\n",
    "\n",
    "print(len(scores))\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels(scoreSavePath, scores, labels, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "EER =\n",
      "\n",
      "   31.9441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/yellowRecording/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii)  for redR: ['R01', 'R05', 'R06', 'R08', 'R09', 'R10', 'R11', 'R16', 'R21', 'R22', 'R23', 'R24', 'R25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R01',\n",
       " 'R05',\n",
       " 'R06',\n",
       " 'R08',\n",
       " 'R09',\n",
       " 'R10',\n",
       " 'R11',\n",
       " 'R16',\n",
       " 'R21',\n",
       " 'R22',\n",
       " 'R23',\n",
       " 'R24',\n",
       " 'R25']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6622\n"
     ]
    }
   ],
   "source": [
    "confKey = redR                  # pass this as a list\n",
    "saveFolder='redRecording'\n",
    "gc=1298\n",
    "\n",
    "# Concatenate all spoof scores\n",
    "green_scores_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, confKey)\n",
    "\n",
    "scores_gen = evalScores[0:gc]   # get genuine scores from eval scores\n",
    "\n",
    "# Put genuine scores on top\n",
    "scores = np.hstack((scores_gen, green_scores_spf))\n",
    "\n",
    "# Make the labels\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(green_scores_spf))))\n",
    "\n",
    "print(len(scores))\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels(scoreSavePath, scores, labels, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "EER =\n",
      "\n",
      "   24.2499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/redRecording/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations on Recording devices**\n",
    "\n",
    "> **greenR** has 6390 total files (first 1298 are genuine and remaining spoof). EER = 33.03\n",
    "\n",
    "> **yellowR** has 2890 total files (first 1298 are genuine and remaining spoof). EER = 31.9\n",
    "\n",
    "> **redR** has 6622 total files (first 1298 are genuine and remaining spoof). EER = 24.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5324"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6622-1298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
