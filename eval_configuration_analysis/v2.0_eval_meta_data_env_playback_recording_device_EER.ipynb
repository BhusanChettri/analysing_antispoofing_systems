{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ASVspoof 2017 v2.0 evaluation dataset meta-data analysis\n",
    "\n",
    "> Color code specifies the level of difficulty\n",
    " 1. **Green** indicates the easiest case to be detected by the system\n",
    " 1. **Yellow** - medium level\n",
    " 1. **Red** - difficult level\n",
    " \n",
    " \n",
    "Using the following end-to-end CNN model (the one we submitted in INTERSPEECH), we compute EER for different qualities of environment, playback and the recording devices\n",
    "\n",
    "> **models_After_ICASSP/InterSpeech2018_v2.0/testing_best_model_with3sec_RELU/keep_0.5_0.5_relurun8 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment ID List :\n",
    "\n",
    "1. Red color - the hardest ones (Total = 5)\n",
    "\n",
    "['E01','E23','E24','E25','E26']\n",
    "\n",
    "2. Green color - the easiest to detect as it is the noisy ones: total = 3\n",
    "\n",
    "['E02','E03','E06']\n",
    "\n",
    "3. Yello color - the medium : total = 18\n",
    "\n",
    "['E04','E05','E07','E08','E09','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yellowE = ['E04','E05','E07','E08','E09','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "greenE = ['E02','E03','E06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redE = ['E01','E23','E24','E25','E26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(yellowE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playback devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Red color, total = 12\n",
    "\n",
    "['P03','P04','P05','P07','P09','P13','P15','P22','P23','P24','P25','P26']\n",
    "\n",
    "> Yellow color, total = 5\n",
    "\n",
    "['P01','P02','P12','P14','P19']\n",
    "\n",
    "> Green color, total = 9\n",
    "\n",
    "['P06','P08','P10','P11','P16','P17','P18','P20','P21']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifiying the counts\n",
    "\n",
    "> The above counts obtained for various devices and environment configuration has been verified with the baseline paper by hector. It matches perfect.\n",
    "\n",
    "> It would be better to also create an scp and protocal list for each of them so in case, if we want to perform analysis for them !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redP = ['P03','P04','P05','P07','P09','P13','P15','P22','P23','P24','P25','P26']\n",
    "yellowP = ['P01','P02','P12','P14','P19']\n",
    "greenP = ['P06','P08','P10','P11','P16','P17','P18','P20','P21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "5\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(redP))\n",
    "print(len(yellowP))\n",
    "print(len(greenP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording device\n",
    "\n",
    "> Red color, total = 13\n",
    "\n",
    "['R01','R05','R06','R08','R09','R10','R11','R16','R21','R22','R23','R24','R25']\n",
    "\n",
    "\n",
    "> Yello color, total = 02\n",
    "\n",
    "['R03','R15']\n",
    "\n",
    "> Green color, total = 10\n",
    "\n",
    "['R02','R04','R07','R12','R13','R14','R17','R18','R19','R20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redR = ['R01','R05','R06','R08','R09','R10','R11','R16','R21','R22','R23','R24','R25']\n",
    "yellowR = ['R03','R15']\n",
    "greenR = ['R02','R04','R07','R12','R13','R14','R17','R18','R19','R20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "2\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(redR))\n",
    "print(len(yellowR))\n",
    "print(len(greenR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay configurations in the evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation dataset has total of 57 replay configurations. Please refer to the Hector odyssey paper to see more details on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evalProt = '/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "protList = []\n",
    "with open(evalProt) as f:\n",
    "    for line in f:\n",
    "        rc = line.strip().split(' ')[4] + ' '+  line.strip().split(' ')[5] + ' ' + line.strip().split(' ')[6]\n",
    "        if rc not in protList and rc != '- - -':\n",
    "            protList.append(rc)\n",
    "    #print(protList)\n",
    "    print(len(protList))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** The eval set has 57 new spoofing configurations ! (verified with the paper )**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Inferring RC-specific file list from the score files\n",
    "\n",
    "> Note that in most of our implementation we used genuineFirstSpoof scp and label list for the evaluation data. Here, we first extracted all the genuine trials in the beginngin and put all the spoof later (like the way trials are organised in the training and the validation sets). There is an immediate need to clean and organize the code structure with proper documentation so that few months later when you read the code you do not get confused on the implementation. Make it a bit organised !\n",
    "\n",
    "> However, we did not create the corresponding protocal file having all the meta-data information for these files. so we will have to carefully infer these meta-data for the score files.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the scp file where we put all genuine files first then spoof\n",
    "\n",
    "evalScp='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/filelists/eval_genFirstSpoof.scp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The original protocal file where genuine and spoof are mixed up\n",
    "\n",
    "evalProt='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The model prediction scores on eval data that use evalScp for spectrogram computation\n",
    "# one example\n",
    "\n",
    "model='/homes/bc305/myphd/stage2/deeplearning.experiment1/CNN3//models_After_ICASSP/InterSpeech2018_v2.0/testing_best_model_with3sec_RELU/'\n",
    "evalScore=model+'/keep_0.5_0.5_relurun8/predictions_original/eval_prediction_new.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With above function we generated a new protocal file for the evaluation data that is based on the genuineFirst\n",
    " spoof next criterion we used during spectrogram computation etc\n",
    " \n",
    "> **new protocal** /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl_genFirstSpoof.txt  \n",
    "\n",
    "> The code for producing the new protocal is here: python_codes/make_genuineFirstspoof_protocal_evalset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_1000010.wav genuine M0035 S06 - - -\n",
      "E_1000018.wav genuine M0023 S01 - - -\n",
      "E_1000074.wav genuine M0031 S09 - - -\n",
      "E_1000102.wav genuine M0030 S09 - - -\n",
      "E_1000123.wav genuine M0029 S10 - - -\n",
      "E_1000151.wav genuine M0028 S07 - - -\n",
      "E_1000161.wav genuine M0034 S10 - - -\n",
      "E_1000179.wav genuine M0025 S05 - - -\n",
      "E_1000192.wav genuine M0029 S08 - - -\n",
      "E_1000217.wav genuine M0028 S05 - - -\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl_genFirstSpoof.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE 1: Environment specific list\n",
    "\n",
    "> **redE** means the hardest one to detect. Please refer to the baseline paper for more details\n",
    "\n",
    "> **yellowE** the medium hard and **green** the easiest ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from eer import find_eers\n",
    "from python_codes import analyse_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreFile = evalScore\n",
    "evalScores = analyse_conf.get_scores(scoreFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evProtocal='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl_genFirstSpoof.txt'\n",
    "evalScpFile='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/filelists/eval_genFirstSpoof.scp'\n",
    "\n",
    "with open(evalScpFile) as f:\n",
    "    evalScp = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)  for greenE ['E02', 'E03', 'E06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1939\n",
      "1939\n"
     ]
    }
   ],
   "source": [
    "# E02\n",
    "\n",
    "confKey = ['E02']\n",
    "saveFolder = 'E02'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   38.7762\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/E02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411\n",
      "1411\n"
     ]
    }
   ],
   "source": [
    "# E03\n",
    "\n",
    "confKey = ['E03']\n",
    "saveFolder = 'E03'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   32.5645\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/E03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583\n",
      "1583\n"
     ]
    }
   ],
   "source": [
    "# E06\n",
    "\n",
    "confKey = ['E06']\n",
    "saveFolder = 'E06'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "    3.3726\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/E06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2337\n",
      "2337\n"
     ]
    }
   ],
   "source": [
    "# Lets do for greenE, the easiest one first: all three in one: E02_E03_E06\n",
    "\n",
    "confKey = ['E02','E03','E06']\n",
    "gc=1298\n",
    "saveFolder = 'greenEnvironment'\n",
    "\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   32.7170\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/greenEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EER stats on greenE, the easiest ones as reported in baseline\n",
    "\n",
    "Using the scores that was produced by our end-to-end model (we reported in Interspeech analysis paper), that showed about 5% EER on DEV and about 29.4 % on the eval set. We computed the EER with respect to the green environment (that is highly noisly and should have been easily detected by the system - thats what the baseline paper argues which use the CQCC features. But our system perform worse still, the reason is fairly simply if we corerelate our INTERSPEECH findings about the classes. )\n",
    "\n",
    "> **E02** has 641 spoof (+1298 genuine), EER = 38.77\n",
    "\n",
    "> **E03** has 113 spoof (+1298 genuine), EER = 32.56\n",
    "\n",
    "> **E06** has 285 spoof (+1298 genuine), EER = 3.3726\n",
    "\n",
    "> **greenEnvironment (E02,E03,E06) ** has 1039 spoof + 1298 genuine), EER = 32.71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii)  for redE = ['E01','E23','E24','E25','E26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2931\n",
      "2931\n"
     ]
    }
   ],
   "source": [
    "# Lets do for redE, the most difficult ones as characterized in the paper\n",
    "\n",
    "confKey = ['E01','E23','E24','E25','E26']\n",
    "gc=1298\n",
    "saveFolder = 'redEnvironment'\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   19.3090\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/redEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Just to double check number of files under redE\n",
    "\n",
    "\n",
    "cat '/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl.txt' | grep E26 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2931"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "748+342+183+182+178+1298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EER stats on redE, the most difficult ones\n",
    "\n",
    "> redE = ['E01','E23','E24','E25','E26']\n",
    "\n",
    "> **redEnvironment ** has total 2931 files (out of which first 1298 are genuine), EER = 19.3. This is very interesting as our system in this configuration seems to ***give better results than the baseline (21.86).*** See Hectors paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii)  for yellowE = ['E04','E05','E07','E08','E09','E10','E11','E12','E13','E14','E15','E16','E17','E18','E19','E20','E21','E22'] \n",
    "\n",
    "The medium difficulty level environment !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10634\n",
      "10634\n"
     ]
    }
   ],
   "source": [
    "# Lets do for redE, the most difficult ones as characterized in the paper\n",
    "\n",
    "confKey = yellowE   # see details of yellowE above\n",
    "gc=1298\n",
    "saveFolder='yellowEnvironment'\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   30.7771\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/yellowEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation on environment specific results\n",
    "\n",
    "> **greenEnvironment (E02,E03,E06) ** has 1039 spoof + 1298 genuine), EER = 32.71\n",
    "\n",
    "> **yellowEnvironment ** has total 10634 files (out of which first 1298 are genuine), EER = 30.77. May be these contain those files that we found earlier in our analysis work that bear genuine file characteristics. Need to double check though.\n",
    "\n",
    "> **redEnvironment ** has total 2931 files (out of which first 1298 are genuine), EER = 19.3. This is very interesting as our system in this configuration seems to ***give better results than the baseline (21.86).*** See Hectors paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1633"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2931-1298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE 2: Playback device specific list\n",
    "\n",
    "> **redE** means the hardest one to detect. Please refer to the baseline paper for more details\n",
    "\n",
    "> **yellowE** the medium hard and **green** the easiest ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)  for greenP ['P06', 'P08', 'P10', 'P11', 'P16', 'P17', 'P18', 'P20', 'P21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P06', 'P08', 'P10', 'P11', 'P16', 'P17', 'P18', 'P20', 'P21']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greenP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5910\n",
      "5910\n"
     ]
    }
   ],
   "source": [
    "# Lets do for greenP, the easiest one first\n",
    "\n",
    "confKey = greenP                  # pass this as a list\n",
    "saveFolder='greenPlayback'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   42.9804\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/greenPlayback/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii)  for yellowP ['P01', 'P02', 'P12', 'P14', 'P19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P01', 'P02', 'P12', 'P14', 'P19']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellowP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2866\n",
      "2866\n"
     ]
    }
   ],
   "source": [
    "confKey = yellowP                  # pass this as a list\n",
    "saveFolder='yellowPlayback'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   22.4272\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/yellowPlayback/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii)  for redP  ['P03', 'P04', 'P05', 'P07', 'P09', 'P13', 'P15', 'P22', 'P23', 'P24', 'P25', 'P26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P03',\n",
       " 'P04',\n",
       " 'P05',\n",
       " 'P07',\n",
       " 'P09',\n",
       " 'P13',\n",
       " 'P15',\n",
       " 'P22',\n",
       " 'P23',\n",
       " 'P24',\n",
       " 'P25',\n",
       " 'P26']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7126\n",
      "7126\n"
     ]
    }
   ],
   "source": [
    "confKey = redP                  # pass this as a list\n",
    "saveFolder='redPlayback'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   17.4575\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/redPlayback/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations on Playback devices**\n",
    "\n",
    "> **greenP** has 5910 total files (first 1298 are genuine and remaining spoof). EER = 42.98\n",
    "\n",
    "> **yellowP** has 2866 total files (first 1298 are genuine and remaining spoof). EER = 22.42\n",
    "\n",
    "> **redP** has 7126 total files (first 1298 are genuine and remaining spoof). EER = 17.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5828"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7126-1298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE 3: Recording device specific list\n",
    "\n",
    "> **redE** means the hardest one to detect. Please refer to the baseline paper for more details\n",
    "\n",
    "> **yellowE** the medium hard and **green** the easiest ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)  for greenR: ['R02', 'R04', 'R07', 'R12', 'R13', 'R14', 'R17', 'R18', 'R19', 'R20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R02', 'R04', 'R07', 'R12', 'R13', 'R14', 'R17', 'R18', 'R19', 'R20']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greenR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6390\n",
      "6390\n"
     ]
    }
   ],
   "source": [
    "confKey = greenR                  # pass this as a list\n",
    "saveFolder='greenRecording'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5092"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6390-1298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   33.0372\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/greenRecording/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii)  for yellowR: ['R03', 'R15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R03', 'R15']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellowR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2890\n",
      "2890\n"
     ]
    }
   ],
   "source": [
    "confKey = yellowR                  # pass this as a list\n",
    "saveFolder='yellowRecording'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   31.9441\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/yellowRecording/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii)  for redR: ['R01', 'R05', 'R06', 'R08', 'R09', 'R10', 'R11', 'R16', 'R21', 'R22', 'R23', 'R24', 'R25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R01',\n",
       " 'R05',\n",
       " 'R06',\n",
       " 'R08',\n",
       " 'R09',\n",
       " 'R10',\n",
       " 'R11',\n",
       " 'R16',\n",
       " 'R21',\n",
       " 'R22',\n",
       " 'R23',\n",
       " 'R24',\n",
       " 'R25']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6622\n",
      "6622\n"
     ]
    }
   ],
   "source": [
    "confKey = redR                  # pass this as a list\n",
    "saveFolder='redRecording'\n",
    "gc=1298\n",
    "\n",
    "scores_spf,scp_spf = analyse_conf.get_config_specific_scores(scoreFile, evProtocal, evalScpFile, confKey)\n",
    "\n",
    "scp_gen = evalScp[0:gc]\n",
    "scores_gen = evalScores[0:gc]\n",
    "\n",
    "scores = np.hstack((scores_gen, scores_spf))\n",
    "labels = np.hstack((np.ones(len(scores_gen)), np.zeros(len(scores_spf))))\n",
    "scps = np.hstack((scp_gen,scp_spf))\n",
    "\n",
    "print(len(scores))\n",
    "print(len(scps))\n",
    "\n",
    "scoreSavePath = model+'/keep_0.5_0.5_relurun8/environment_wise/'\n",
    "\n",
    "# Save the score file and label file\n",
    "analyse_conf.save_scores_labels_scps(scoreSavePath, scores, labels, scps, saveFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2018 The MathWorks, Inc.\n",
      "                   R2018a (9.4.0.813654) 64-bit (glnxa64)\n",
      "                             February 23, 2018\n",
      "\n",
      " \n",
      "To get started, type one of these: helpwin, helpdesk, or demo.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "in get_eer\n",
      "\n",
      "EER =\n",
      "\n",
      "   24.2499\n",
      "\n",
      "Writing to the file !!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python eer.py keep_0.5_0.5_relurun8/environment_wise/redRecording/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations on Recording devices**\n",
    "\n",
    "> **greenR** has 6390 total files (first 1298 are genuine and remaining spoof). EER = 33.03\n",
    "\n",
    "> **yellowR** has 2890 total files (first 1298 are genuine and remaining spoof). EER = 31.9\n",
    "\n",
    "> **redR** has 6622 total files (first 1298 are genuine and remaining spoof). EER = 24.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5324"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6622-1298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input: a) pass the score file and the new protocal filelist corresponding to these scores\n",
    "#        b) search configuration (could be environment, playback etc)\n",
    "\n",
    "# Output:\n",
    "#        a) configuration specific score files along with labels\n",
    "#        b) should i write this in a file or find a way to pass this into EER computing script\n",
    "#        c) return the EER for this configuration\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_config_specific_scores11(scoreFile, protocal, scpFile, confKeys):\n",
    "    \n",
    "    '''\n",
    "    This function returns the scores corresponding to replay config parameter in confkey\n",
    "    Inputs\n",
    "       1) scoreFile: this is a score file as per genuineFirstspoof list which is often used in my work\n",
    "       2) protocal:  denotes the genuineFirstspoof protocal file we created using genuineFirstSpoof.scp file\n",
    "       2) scpFile :  this is the scp file that holds the path to the audio files.\n",
    "       3) confKeys : could be a list of configuration key based on which we want to search the spoof scores.\n",
    "                     This is basically used to create sub-score list using config and evaluate the EER\n",
    "                     \n",
    "    NOTE: if you use it for dev and train set, you need to be careful passing correct files !                 \n",
    "                     \n",
    "    Output:\n",
    "       1) list of scores that match keys in the confKeys list\n",
    "       2) list of scps that match keys in the confKeys list. This is to create filepath for all those spoof files\n",
    "          This will be useful for analysing later, may be using slime !\n",
    "                     \n",
    "    '''\n",
    "    \n",
    "    # Read the scp file into a list\n",
    "    with open(scpFile) as f:\n",
    "        scps = [line.strip() for line in f]\n",
    "                \n",
    "    # Read scores and put into a list\n",
    "    with open(scoreFile) as f:\n",
    "        scores = [line.strip() for line in f]\n",
    "        \n",
    "    # Read protocal files into a list\n",
    "    with open(protocal) as f:\n",
    "        prots = [line.strip() for line in f]  \n",
    "    \n",
    "   \n",
    "    # Loop over all the items/keys in the confkey and append all the corresponding scores    \n",
    "    count=0\n",
    "    confKey_scores = list()\n",
    "    confKey_scps = list()\n",
    "    \n",
    "    for confKey in confKeys:\n",
    "        # find all those files that match confKey in the protocal, and get the respective scores    \n",
    "        \n",
    "        for i in range(0,len(prots)):\n",
    "            if confKey in prots[i]:            \n",
    "                # append the spoof scores that match the confKey\n",
    "                confKey_scores.append(scores[i])\n",
    "                confKey_scps.append(scps[i])\n",
    "                \n",
    "                count += 1\n",
    "        \n",
    "    #print('Total spoof in this config is: ', count)    \n",
    "    \n",
    "    return confKey_scores, confKey_scps\n",
    "    # Just return all the scores and scps that match confKey\n",
    "    \n",
    "    \n",
    "def save_scores_labels_scps(savePath,scores,labels,scps,confKey):\n",
    "    '''\n",
    "    This function is used to save the scores and labels as per confkeys\n",
    "    \n",
    "    Inputs\n",
    "         1) savePath: where to save the scores? \n",
    "         2) scores and labels are corresponding to the confKey\n",
    "         3) scps: which will have first genuine and then all those spoof ones that matched confKey\n",
    "    \n",
    "    '''\n",
    "              \n",
    "    #First create directories related to confKey for saving scores and labels\n",
    "    saveDir = savePath+'/'+confKey\n",
    "            \n",
    "    make_directory(saveDir)    \n",
    "        \n",
    "    with open(saveDir+'/score.txt', 'w') as f:\n",
    "        for s in scores:\n",
    "            f.write(str(s)+'\\n')\n",
    "\n",
    "    with open(saveDir+'/labels.txt','w') as f:\n",
    "        for l in labels:\n",
    "            if l == 1.0:\n",
    "                out='genuine'\n",
    "            else:\n",
    "                out='spoof'\n",
    "                \n",
    "            f.write(out+'\\n')   \n",
    "            \n",
    "    with open(saveDir+'audio.scp','w') as f:\n",
    "        for path in scps:\n",
    "            f.write(path+'\\n')\n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Find meta-data details on Training and the Development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainProt='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_train.trn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_1001701.wav spoof M0018 S04 E03 P08 R04\n",
      "D_1001702.wav spoof M0014 S09 E03 P08 R04\n",
      "D_1001703.wav spoof M0016 S01 E03 P08 R04\n",
      "D_1001704.wav spoof M0018 S01 E03 P08 R04\n",
      "D_1001705.wav spoof M0018 S02 E03 P08 R04\n",
      "D_1001706.wav spoof M0016 S07 E03 P08 R04\n",
      "D_1001707.wav spoof M0011 S03 E03 P08 R04\n",
      "D_1001708.wav spoof M0011 S02 E03 P08 R04\n",
      "D_1001709.wav spoof M0012 S08 E03 P08 R04\n",
      "D_1001710.wav spoof M0012 S06 E03 P08 R04\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#cat /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_train.trn.txt | tail\n",
    "\n",
    "cat /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_dev.trl.txt | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_config(protocal, key):\n",
    "    '''\n",
    "    Inputs:\n",
    "    > get the count of key in the file. Key could be environment, playback device, replay device or replay config\n",
    "    > protocal is the protocal file of the dataset (train,dev or eval)\n",
    "    \n",
    "    Outputs:\n",
    "    > the number of files with matching key\n",
    "    '''\n",
    "    if key == 'E':\n",
    "        n=4\n",
    "    elif key == 'P':\n",
    "        n=5\n",
    "    elif key == 'R':\n",
    "        n=6\n",
    "    \n",
    "    confList = list()\n",
    "    with open(protocal) as f:\n",
    "        for line in f:                                  \n",
    "            if key != 'EPR':\n",
    "                d = line.strip().split(' ')[n]\n",
    "            else:\n",
    "                d = line.strip().split(' ')[4] + ' ' + line.strip().split(' ')[5] + ' ' + line.strip().split(' ')[6]                                                \n",
    "            \n",
    "            if d not in confList:\n",
    "                if d == '-' or d == '- - -':\n",
    "                    continue\n",
    "                else:\n",
    "                    confList.append(d)                                                                                                    \n",
    "                \n",
    "    return confList\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E03', 'E21']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_config(trainProt, 'E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P01', 'P02', 'P03']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_config(trainProt, 'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R01']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_config(trainProt, 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E03 P01 R01', 'E21 P02 R01', 'E21 P03 R01']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_config(trainProt, 'EPR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On training set, version 2.0\n",
    "\n",
    "> ** Environments** : 2 environements. ['E03', 'E21']\n",
    "\n",
    "> **Playback devices**: 3 playback devices. ['P01', 'P02', 'P03']\n",
    "\n",
    "> **Recording devices**: 1 recording devices. ['R01']\n",
    "\n",
    "> **Replay configurations**: 3. ['E03 P01 R01', 'E21 P02 R01', 'E21 P03 R01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On development set, version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "devProt='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_dev.trl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E16', 'E06', 'E18', 'E04', 'E05', 'E03']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_config(devProt, 'E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P07', 'P09', 'P05', 'P06', 'P01', 'P08']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_config(devProt, 'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R06', 'R05', 'R07', 'R03', 'R02', 'R01', 'R04']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_config(devProt, 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E16 P07 R06',\n",
       " 'E16 P07 R05',\n",
       " 'E16 P07 R07',\n",
       " 'E06 P09 R06',\n",
       " 'E06 P09 R05',\n",
       " 'E06 P09 R07',\n",
       " 'E18 P05 R03',\n",
       " 'E04 P06 R02',\n",
       " 'E05 P01 R01',\n",
       " 'E03 P08 R04']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_config(devProt, 'EPR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On development set, version 2.0\n",
    "\n",
    "> ** Environments** : 6 environements. ['E16', 'E06', 'E18', 'E04', 'E05', 'E03']\n",
    "\n",
    "> **Playback devices**: 6 playback devices. ['P07', 'P09', 'P05', 'P06', 'P01', 'P08']\n",
    "\n",
    "> **Recording devices**: 7 recording devices. ['R06', 'R05', 'R07', 'R03', 'R02', 'R01', 'R04']\n",
    "\n",
    "> **Replay configurations**: 10. ['E16 P07 R06', 'E16 P07 R05', 'E16 P07 R07', 'E06 P09 R06', 'E06 P09 R05', 'E06 P09 R07', 'E18 P05 R03', 'E04 P06 R02', 'E05 P01 R01', 'E03 P08 R04']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the evaluation set, version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evalProt='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl_genFirstSpoof.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_1000010.wav genuine M0035 S06 - - -\n",
      "E_1000018.wav genuine M0023 S01 - - -\n",
      "E_1000074.wav genuine M0031 S09 - - -\n",
      "E_1000102.wav genuine M0030 S09 - - -\n",
      "E_1000123.wav genuine M0029 S10 - - -\n",
      "E_1000151.wav genuine M0028 S07 - - -\n",
      "E_1000161.wav genuine M0034 S10 - - -\n",
      "E_1000179.wav genuine M0025 S05 - - -\n",
      "E_1000192.wav genuine M0029 S08 - - -\n",
      "E_1000217.wav genuine M0028 S05 - - -\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/protocol_V2/ASVspoof2017_V2_eval.trl_genFirstSpoof.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count_config(evalProt, 'E')  # total 24\n",
    "sum([1 for i in count_config(evalProt, 'E')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count_config(evalProt, 'P')  # total 23\n",
    "sum([1 for i in count_config(evalProt, 'P')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count_config(evalProt, 'R')  # total 24\n",
    "sum([1 for i in count_config(evalProt, 'R')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count_config(evalProt, 'EPR')  # total \n",
    "sum([1 for i in count_config(evalProt, 'EPR')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On evaluation set, version 2.0\n",
    "\n",
    "> ** Environments** : 24 environements. ['E19','E14','E12','E18','E11','E20','E07','E15','E17','E02','E01','E08','E13', 'E21','E26','E22','E25','E24','E10','E16','E23','E06','E09','E03']\n",
    "\n",
    "> **Playback devices**: 23 playback devices. ['P22','P03','P16','P05','P26','P10','P21','P19','P12','P15','P20','P14', 'P24','P13','P23','P17','P07','P25','P09','P04','P18','P08','P11']\n",
    "\n",
    "> **Recording devices**: 24 recording devices. ['R22','R04','R11','R03','R16','R15','R18','R10','R25','R13','R14','R01',\n",
    " 'R24','R08','R23','R19','R06','R17','R09','R21','R05','R07','R20','R12']\n",
    "\n",
    "> **Replay configurations**: 57 replay configurations. Test set has all replay configurations ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Total environments = 26\n",
    "# Total Playback = 26\n",
    "# Total recording = 25\n",
    "# Total Replay configurations = 57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap between training and eval set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Environment overlap**: ['E03', 'E21'] in train set appears in test set. E03 is balcony environment, which is assumed to be easy to detect cause it would be very noisy. E21 is office 09  environment recorded in office condition which will have some noise. According to hector paper these two environments are categorised under easy condition.\n",
    "\n",
    "> **Playback devices**:  3 playback devices. ['P01', 'P02', 'P03'] in training set. ***P03 appears in eval*** P01 and P03 are easy categories (yellow) and P03 hard category (red). See hector paper for details on color coding.\n",
    "\n",
    "> **Recording devices**:  Only 'R01' appears in training set. ***R01 appears*** in eval set too. R01 is the hard category\n",
    "\n",
    "> **Replay configuration**. 3 RC appear in training set ['E03 P01 R01', 'E21 P02 R01', 'E21 P03 R01']. Configuration ***'E21 P03 R01'*** (which is called RC28, see hector paper) is repeated in eval set. RC28 has yellow environment and red playback and recording device.\n",
    "\n",
    "*** Note:*** Green and yellow color signifies that replayed signals should be relatively easy to detect. While red color signifies that the signals are pretty hard to be distinguished as spoof signals cause they leave no discriminative cue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap between development and eval set\n",
    "\n",
    "> **Environment overlap**:  6 environements. ['E16', 'E06', 'E18', 'E04', 'E05', 'E03'] in dev set. Except E04 and E05 all other appear in test set. All these environments comes under green and yellow conditions.\n",
    "\n",
    "> **Playback devices**: 6 playback devices. ['P07', 'P09', 'P05', 'P06', 'P01', 'P08'] in dev set. Except P06,P01 all other appear in test set.\n",
    "\n",
    "> **Recording devices**: 7 recording devices. ['R06', 'R05', 'R07', 'R03', 'R02', 'R01', 'R04'] in dev set. Except R02 all other appear in test set.\n",
    "\n",
    "> **Replay configurations**:  10 rc's ['E16 P07 R06', 'E16 P07 R05', 'E16 P07 R07', 'E06 P09 R06', 'E06 P09 R05', 'E06 P09 R07', 'E18 P05 R03', 'E04 P06 R02', 'E05 P01 R01', 'E03 P08 R04'] in dev set.\n",
    "\n",
    "> Repeated in eval set are\n",
    "\n",
    "    'E16 P07 R06' = RC40\n",
    "    'E16 P07 R05' = RC51\n",
    "    'E16 P07 R07' = RC31\n",
    "    'E06 P09 R06' = RC43\n",
    "    'E06 P09 R05' = RC54\n",
    "    'E06 P09 R07' = RC27\n",
    "    'E18 P05 R03' = RC29\n",
    "\n",
    "\n",
    "> Not repeated\n",
    "\n",
    "    'E04 P06 R02'\n",
    "    'E05 P01 R01'\n",
    "    'E03 P08 R04'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap between the training and development set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
