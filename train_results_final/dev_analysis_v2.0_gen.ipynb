{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Genuine files in Train set - version 2.0 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3014\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "ls /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_train/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the prediction file\n",
    "# Predictions file that has three columns: genuin, spoofed prob and log likehood ratio\n",
    "\n",
    "pred_file='model_3sec_relu_0.5_run8/predictions/train_prediction.txt'\n",
    "#new_pred_file = 'model_3sec_relu_0.5_run8/predictions/train_prediction_with_index.txt'\n",
    "new_pred_file = 'predictions/train_prediction_with_index.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open(pred_file) as f, open(new_pred_file,'w') as f2:\\n    i=0\\n    for line in f:\\n        f2.write(str(i)+' '+ line)\\n        #print(i)\\n        i+=1    \\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new prediction file where we write index (starting from 0) as the first column\n",
    "# This way things become much easier\n",
    "\n",
    "'''\n",
    "with open(pred_file) as f, open(new_pred_file,'w') as f2:\n",
    "    i=0\n",
    "    for line in f:\n",
    "        f2.write(str(i)+' '+ line)\n",
    "        #print(i)\n",
    "        i+=1    \n",
    "'''\n",
    "\n",
    "# Above code is just used to append file index in the prediction file\n",
    "# Careful when re-running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.999606 0.000393718 7.83948\n",
      "1 0.998261 0.00173903 6.35269\n",
      "2 0.999865 0.000135052 8.90972\n",
      "3 0.997186 0.00281445 5.87017\n",
      "4 0.999346 0.000653539 7.33245\n",
      "5 0.99927 0.000729632 7.22224\n",
      "6 0.993516 0.00648415 5.03189\n",
      "7 0.99779 0.00220973 6.11267\n",
      "8 0.999441 0.000558659 7.48941\n",
      "9 0.995694 0.00430576 5.44349\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "#cat model_3sec_relu_0.5_run8/predictions/train_prediction_with_index.txt | head\n",
    "cat predictions/train_prediction_with_index.txt | head\n",
    "\n",
    " #Check the new prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate out predictions of genuine and spoofed files\n",
    "# Training set has first 1507 as genuine and remaining 1507 as spoofed\n",
    "\n",
    "all_predictions = []\n",
    "all_gens = []\n",
    "all_spoofs = []\n",
    "\n",
    "with open(new_pred_file) as f:\n",
    "    all_predictions = [line.strip() for line in f]\n",
    "    \n",
    "all_gen_predictions = all_predictions[:1507]                 # first 1507 files are genuine\n",
    "all_spf_predictions = all_predictions[1507:]                 # files from 1507 onwards are spoofed examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1507\n",
      "1507\n"
     ]
    }
   ],
   "source": [
    "print(len(all_gen_predictions))\n",
    "print(len(all_spf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfname='index_file_list/allGenIndexList_TP.txt'\\ncount=0\\nsplitIndex = 1     # 1 for genuine and 2 for spoofed (the third column)\\npredList = all_gen_predictions   # use all_spf_predictions for spoofed case\\n\\n#print(len(predList))\\n\\nwith open(fname,'w') as f2:\\n    for items in predList:\\n        prob = items.strip().split(' ')[splitIndex]   \\n        if float(prob) > 0.9:\\n            #print(items)\\n            count+=1\\n            f2.write(items+'\\n')            \\nprint(count)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect all genuine file for which genuine class got probability > 90%\n",
    "# the strongly correctly classified cases\n",
    "\n",
    "'''\n",
    "fname='index_file_list/allGenIndexList_TP.txt'\n",
    "count=0\n",
    "splitIndex = 1     # 1 for genuine and 2 for spoofed (the third column)\n",
    "predList = all_gen_predictions   # use all_spf_predictions for spoofed case\n",
    "\n",
    "#print(len(predList))\n",
    "\n",
    "with open(fname,'w') as f2:\n",
    "    for items in predList:\n",
    "        prob = items.strip().split(' ')[splitIndex]   \n",
    "        if float(prob) > 0.9:\n",
    "            #print(items)\n",
    "            count+=1\n",
    "            f2.write(items+'\\n')            \n",
    "print(count)\n",
    "'''\n",
    "# Careful when you re-run this code. It is use to take files with 90% probability to do analysis using slime !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat index_file_list/allGenIndexList_TP.txt | wc -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total genuine files we got with > 90% probability is = 1472 (out of 1507)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Take the top 10 genuine confident correctly classified files - high scores\n",
    "\n",
    "                               Gen prob     Spf Prob      Log Liklihood ratio\n",
    "    \n",
    "       \n",
    "**** Note when accessing via list or array, index 575 should be called as 574       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analysing Time: The top two components from SLIME - True positive Genuine case\n",
    "\n",
    "Note, that under time analysis, we have cut our input spectrogram into 10 different temporal components/segments, where each segment correpsonds to\n",
    "\n",
    "> ***300 mili seconds***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "time.png",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show how we cut the spectrogram in timexfrequency\n",
    "\n",
    "Image(\"time.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the temporal explanantion file produced by slime\n",
    "\n",
    "file = 'top_two_explanation_indices/time/gen_TP_box.txt'    #using box for spectrogram computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Make sure to use the correct spectrogram for two cases !! ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cat top_two_explanation_indices/time/gen_TP_box.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top1_top2_list(file):\n",
    "    with open(file) as f:\n",
    "        top1 = [int(line.strip().split(' ')[0]) for line in f]\n",
    "    with open(file) as f:  \n",
    "        top2 = [int(line.strip().split(' ')[1]) for line in f]\n",
    "    return top1, top2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top1 and top2 in seperate list\n",
    "top1, top2 = get_top1_top2_list(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472\n",
      "1472\n"
     ]
    }
   ],
   "source": [
    "print(len(top1))\n",
    "print(len(top2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_counts(datalist, key):\n",
    "    count=0\n",
    "    for i in range(0,len(datalist)):\n",
    "        if datalist[i] == key:\n",
    "            count+=1\n",
    "    return count            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_component_distribution(comps, predList, order):\n",
    "    print('Printing component weigting distribution for Top:', order)\n",
    "    for i in comps:\n",
    "        print('Component ' + str(i) + ' : ' + str(get_counts(predList, i)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the top1 components - given by SLIME (Time segmentation) - Genuine Class\n",
    "\n",
    "   \n",
    "> Using only those files with 90% above genuine class probability. The pattern is same (total files this time we used is 685 earlier we used 766 that include genuine files with proba 60 % also\n",
    "\n",
    "    Component 0 : 20\n",
    "    Component 1 : 2\n",
    "    Component 2 : 240\n",
    "    Component 3 : 551\n",
    "    Component 4 : 81\n",
    "    Component 5 : 316\n",
    "    Component 6 : 200\n",
    "    Component 7 : 54\n",
    "    Component 8 : 3\n",
    "    Component 9 : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comps=[0,1,2,3,4,5,6,7,8,9]    # In time we have 10 segments/components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing component weigting distribution for Top: 1\n",
      "Component 0 : 20\n",
      "Component 1 : 2\n",
      "Component 2 : 240\n",
      "Component 3 : 551\n",
      "Component 4 : 81\n",
      "Component 5 : 316\n",
      "Component 6 : 200\n",
      "Component 7 : 54\n",
      "Component 8 : 3\n",
      "Component 9 : 5\n"
     ]
    }
   ],
   "source": [
    "# Print distribution on top1\n",
    "\n",
    "order = 1\n",
    "print_component_distribution(comps, top1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick 5 audio files for hearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random_5_ids = [506,480,757,208,481]   # Genuine True positive, >90% probability\n",
    "#add 1 to each to access correct file index in real world\n",
    "\n",
    "#random_5_ids = [507,481,758,209,482]   # Genuine True positive, >90% probability\n",
    "#base='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#ls /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_dev/D_1000481.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#cp /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_dev/D_1000482.wav audio_files_hearing/genuine_TP/\n",
    "\n",
    "#play audio_files_hearing/genuine_TP/D_1000507.wav\n",
    "#ls audio_files_hearing/genuine_TP/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analysing Freq: The top two components from SLIME - True positive Genuine case\n",
    "\n",
    "Note, that under frequency analysis, we have cut our input spectrogram into 8 different frequency components/segments, where each segment correpsonds to\n",
    "\n",
    "> ***1000 Hz frequency***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "frequency.png",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show how we cut the spectrogram in timexfrequency\n",
    "\n",
    "Image(\"frequency.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the spectral explanation file\n",
    "\n",
    "file = 'top_two_explanation_indices/freq/gen_TP_box.txt'   #using signal box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#cat 'top_two_explanation_indices/freq/gen_TP.txt' | head\n",
    "\n",
    "# the top two components 7 6 dominates the explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top1 and top2 in seperate list\n",
    "top1, top2 = get_top1_top2_list(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472\n",
      "1472\n"
     ]
    }
   ],
   "source": [
    "print(len(top1))\n",
    "print(len(top2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 component distribution - Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comps = [0,1,2,3,4,5,6,7]   # in Frequency we have 8 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing component weigting distribution for Top: 1\n",
      "Component 0 : 0\n",
      "Component 1 : 0\n",
      "Component 2 : 0\n",
      "Component 3 : 0\n",
      "Component 4 : 0\n",
      "Component 5 : 92\n",
      "Component 6 : 6\n",
      "Component 7 : 1374\n"
     ]
    }
   ],
   "source": [
    "# Print distribution on top1\n",
    "\n",
    "order = 1\n",
    "print_component_distribution(comps, top1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With 685 files chosen with > 90% probability, distribution of TOP1 explanation (its same as before)\n",
    "\n",
    "    Component 0 : 2\n",
    "    Component 1 : 0\n",
    "    Component 2 : 0\n",
    "    Component 3 : 0\n",
    "    Component 4 : 0\n",
    "    Component 5 : 9\n",
    "    Component 6 : 0\n",
    "    Component 7 : 674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get those file index having component8 in top explanation (as for frequency we get top explanation at index8)\n",
    "top = 7\n",
    "\n",
    "fname='index_file_list/allGenIndexList_TP.txt'\n",
    "with open(fname) as f:\n",
    "    #file_idxs = [int(line.strip().split(' ')[0]) for line in f]\n",
    "    file_idxs = [line.strip() for line in f]\n",
    "    \n",
    "#Write the top component index to the file and further analysis\n",
    "fname='top_two_explanation_indices/freq/topExplanation_list_gen_TP_box.txt'\n",
    "id8_indexFiles=list() \n",
    "\n",
    "f=open(fname,'w')\n",
    "for i in range(len(top1)):\n",
    "    if top1[i]==top:    # if top explanation index is 3 (which is actually 4)\n",
    "        id8_indexFiles.append(file_idxs[i])\n",
    "        #print(file_idxs[i].strip().split(' ')[0])\n",
    "        #print(file_idxs[i])        \n",
    "        #f.write(str(file_idxs[i])+'\\n')\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Observations**\n",
    "\n",
    "> We find that frequency component 8 corresponding to 7000-8000 Hz, seems to get maximally activated for making the prediction. Still, its hard to make any signifant conclusion about a class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick 5 audio files for hearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random_5_ids = [571,455,387,202,708]   # Genuine True positive, >90% probability\n",
    "#add 1 to each to access correct file index in real world\n",
    "\n",
    "#random_5_ids = [572,456,388,203,709]   # Genuine True positive, >90% probability\n",
    "#base='/homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#cp /homes/bc305/myphd/datasets/ASVSpoof2017_v2.0/ASVspoof2017_V2_dev/D_1000709.wav audio_files_hearing/genuine_TP/\n",
    "\n",
    "#play audio_files_hearing/genuine_TP/D_1000507.wav\n",
    "#ls audio_files_hearing/genuine_TP/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# On Training set\n",
    "\n",
    "The training set has 1507 genuine and spoofed audio files. Note that we have used only the training dataset to train our end-to-end CNN system. On training set our model show an EER of about 0.26. Therefore, we analyse what has the CNN learned/exploited about these two classes of data.\n",
    "\n",
    "\n",
    "## What CNN has exploited about Genuine signal from the training data ?\n",
    "\n",
    "We first collect all the genuine audio files that has been strongly correctly classified with more than 90% probability by the CNN model. We find 1472 (out of 1507). Next, we run SLIME algorithm on these audio files and generate temporal and spectral explanations.\n",
    "\n",
    "Distribution of **temporal explanation** for \n",
    "\n",
    "    Component 0 : 20\n",
    "    Component 1 : 2\n",
    "    Component 2 : 240\n",
    "    Component 3 : 551\n",
    "    Component 4 : 81\n",
    "    Component 5 : 316\n",
    "    Component 6 : 200\n",
    "    Component 7 : 54\n",
    "    Component 8 : 3\n",
    "    Component 9 : 5\n",
    "\n",
    "   \n",
    "Distribution of **Spectral explanation**\n",
    "\n",
    "    Component 0 : 0\n",
    "    Component 1 : 0\n",
    "    Component 2 : 0\n",
    "    Component 3 : 0\n",
    "    Component 4 : 0\n",
    "    Component 5 : 92\n",
    "    Component 6 : 6\n",
    "    Component 7 : 1374\n",
    "    \n",
    "    \n",
    "> How do we represent the above distribution? Table or bar-chart or what? First we need to be showing this on the training data to claim what the CNN has learned from the training data. Then we move on to prove the hypothesis on the development and the evaluation data.\n",
    "\n",
    "    \n",
    "On average, the explanation distribution show strong emphasis in the center of the 3 second audio signal (between 0.9 - 2.1secs). This corresponds to the region in audio signal where speech onset is detected. We find that majority of genuine audio files in the development data has non-speech frames in the beginning and speech onset in majority of cases starts somewhere around 900ms. On the spectral explanation distribution, it gives a very clear explanation that the model is looking at high frequency information above 7 kHz. \n",
    "\n",
    "> Therefore, what **conclusion** we get from above is that the model is identifying a file as genuine using information above 7 kHz mostly in the middle of the audio signal.\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
